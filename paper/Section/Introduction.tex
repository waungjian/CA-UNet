\section{Introduction}

Due to the great success of deep learning, CNNs, especially Fully Convolutional Networks (FCN)~\cite{long2015fully}, have been widely used in the field of medical image segmentation. Based on the FCN, U-Net~\cite{ronneberger2015u}, as the pioneer of medical image segmentation, has attracted the attention of many researchers with its symmetrical encoder-decoder framework. In this topology structure, the encoder is used to extract feature information of different scales from images, while the decoder fuses the feature information extracted by the encoder via cross-layer connections and restores it to the original input image size, making pixel-level semantic predictions. This network structure has achieved effective fusion of fine-grained information and semantic information in small-sample data, and achieved great success in various medical image segmentation tasks. In order to alleviate the loss of spatial information caused by downsampling, U-Net++~\cite{zhou2019unet++} and U-Net3+~\cite{huang2020unet} networks based on multi-scale information fusion mechanism and deep supervision have been proposed successively. These methods learn hierarchical representations from feature maps aggregated at multiple scales, thus obtaining more accurate segmentation results.In addition, the DeepLab~\cite{liang2015semantic,chen2017deeplab,chen2017rethinking,chen2018encoder} series of networks combined with Atrous convolution designed a multi-scale objective robust algorithm based on Atrous Spatial Pyramid Pooling (ASPP).
% many fields such as

Although CNN have powerful feature extraction capabilities, due to the inherent locality of their convolution operations, CNN-based medical image semantic segmentation networks still show certain limitations in modelling visual receptive fields and long-range dependencies. The emergence of the visual attention mechanism provides a novel and effective path to enhance the long-range modeling capacity of CNNs. Attention U-Net~\cite{oktay2018attention} first introduced the gated attention mechanism into the U-Net network, allowing the network to learn the complex relations between features across different scales and hierarchical levels. In the same year, Woo et al.~\cite{woo2018cbam} effectively combined channel attention (CA) and spatial attention (SA) to propose a novel and lightweight convolution block attention module (CBAM). This module can be flexibly embedded into deep CNNs for various application scenarios and tasks without introducing extra computational or parameter load. A large number of experiments have demonstrated the effectiveness and generalization capabilities of CBAM in tasks such as image classification and image segmentation.

The self-attention mechanism based on Transformer~\cite{vaswani2017attention} has achieved great success in the field of natural language processing(NLP) due to its exceptional ability to model long-range dependencies, paving a new research path in the field of computer vision. In~\cite{dosovitskiy2020image}, Visual Transformer (ViT) was proposed and applied for image classification tasks. It performs equivalently to CNN-based classification networks when pre-trained on large-scale datasets, showcasing the powerful ability of Transformers in global context modeling. To enhance ViT's local perception capabilities and handle super-resolution images, researchers designed the Swin Transformer~\cite{liu2021swin}, which has been successfully applied in fields like image object detection and semantic segmentation. A series of studies have adopted Transformers in the field of medical image segmentation, seeking to replace CNNs~\cite{cao2022swin,huang2022missformer}. However, due to the lack of key inductive biases inherent to CNNs, such as translation invariance and locality, its ability to extract spatial detail features is limited. Based on the complementary strengths of CNNs and ViTs, a series of hybrid U-shaped topologies based on CNN-ViT have been proposed for medical image segmentation tasks~\cite{chen2021transunet,zhang2021transfuse,naderi2022focal,lan2024brau}. A potential shortfall of these networks, however, is the lack of integration of the complementary strengths of both at each level of feature extraction modules, rendering them unable to effectively establish dependencies between local and global information. The exceptional performance of CMTs~\cite{guo2022cmt} in various visual tasks presents us with a novel method of hierarchical fusion between CNN-ViT.

Motivated by the success of the CMT architecture and the Swin Transformer~\cite{guo2022cmt,liu2021swin}, we propose CA-UNet for 2D medical image segmentation, including encoders, bottleneck, decoders, and full-scale skip connections. We combined the window attention mechanism unit in Swin Transformer with depthwise convolution (DwConv) based on a dynamic weight allocation mechanism to construct a parallel heterogeneous module (CA block). The encoders, bottleneck, and decoders are all built based on the CA block. First, spatial and structural information extraction is carried out on the input image stem architecture~\cite{he2019bag} and then input to the encoder for deep representation learning. The encoder obtains multi-scale feature maps based on a downsampling structure with residual architecture. Through the decoder and a full-scale skip connection module based on convolutional attention, the deep feature map extracted is fused with multi-scale features of different resolutions on the encoder path to achieve precise positioning. Numerous experiments on multi-organ and heart segmentation datasets have demonstrated the superiority of this method. Specifically, our contributions can be summarized as: (1) providing a novel U-shaped symmetric encoder-decoder architecture based on CNN and Transformer for medical image segmentation tasks based on parallel heterogeneous CA blocks; (2) a downsampling module based on residual architecture(Res block), which enhances key detail features of the downsampled feature map, further highlighting the hierarchical relationship of features; (3) a full-scale skip connection module based on CBAM, which allows the network to autonomously learn the importance of the encoder's feature information for decoders at different levels.
